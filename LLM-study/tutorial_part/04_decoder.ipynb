{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal LM: self-attention + ffn\n",
    "\n",
    "![decoder](./../image/Decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([3, 4])\n",
      "mask:\n",
      " tensor([[[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0],\n",
      "          [1, 0, 0, 0]]]])\n",
      "0th x shape torch.Size([3, 4, 64])\n",
      "1th x shape torch.Size([3, 4, 64])\n",
      "2th x shape torch.Size([3, 4, 64])\n",
      "3th x shape torch.Size([3, 4, 64])\n",
      "4th x shape torch.Size([3, 4, 64])\n",
      "torch.Size([3, 4, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0603, 0.1562, 0.0590, 0.0412, 0.1855, 0.0615, 0.1033, 0.0349,\n",
       "          0.1118, 0.0742, 0.0667, 0.0453],\n",
       "         [0.0366, 0.1617, 0.0741, 0.0388, 0.2317, 0.0572, 0.0811, 0.0302,\n",
       "          0.0665, 0.0848, 0.1008, 0.0365],\n",
       "         [0.0418, 0.1505, 0.0955, 0.0324, 0.2099, 0.0725, 0.0871, 0.0310,\n",
       "          0.0683, 0.0802, 0.0934, 0.0373],\n",
       "         [0.0404, 0.1444, 0.0796, 0.0364, 0.2727, 0.0623, 0.0735, 0.0293,\n",
       "          0.0661, 0.0742, 0.0886, 0.0325]],\n",
       "\n",
       "        [[0.0464, 0.0750, 0.1038, 0.0929, 0.2257, 0.0752, 0.0397, 0.0793,\n",
       "          0.0719, 0.0613, 0.0437, 0.0852],\n",
       "         [0.0468, 0.0684, 0.1046, 0.1067, 0.2086, 0.0758, 0.0434, 0.0797,\n",
       "          0.0793, 0.0527, 0.0456, 0.0885],\n",
       "         [0.0445, 0.0625, 0.0895, 0.0616, 0.2889, 0.0768, 0.0429, 0.0754,\n",
       "          0.0620, 0.0664, 0.0350, 0.0945],\n",
       "         [0.0498, 0.0833, 0.0996, 0.1033, 0.2040, 0.0784, 0.0426, 0.0722,\n",
       "          0.0733, 0.0513, 0.0512, 0.0910]],\n",
       "\n",
       "        [[0.0940, 0.0516, 0.0455, 0.1461, 0.0885, 0.0735, 0.0658, 0.0464,\n",
       "          0.0736, 0.2177, 0.0357, 0.0617],\n",
       "         [0.0461, 0.0584, 0.0416, 0.0923, 0.1053, 0.0724, 0.1136, 0.0559,\n",
       "          0.0756, 0.2216, 0.0396, 0.0776],\n",
       "         [0.0549, 0.0598, 0.0439, 0.1112, 0.0911, 0.0721, 0.0989, 0.0387,\n",
       "          0.0621, 0.2570, 0.0456, 0.0648],\n",
       "         [0.0775, 0.0629, 0.0435, 0.1377, 0.0816, 0.0801, 0.0914, 0.0430,\n",
       "          0.0841, 0.1951, 0.0364, 0.0668]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim : int, head_num : int, attention_dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num # 整除关系\n",
    "        \n",
    "        # layer (mha, ffn)\n",
    "        # mha\n",
    "        self.q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout_att = nn.Dropout(attention_dropout_rate)\n",
    "        self.att_ln = nn.LayerNorm(hidden_dim, eps=1e-7)\n",
    "        \n",
    "        # ffn\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4) # (swishGLU, ) 8 / 3\n",
    "        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dropout_ffn = nn.Dropout(1e-1)\n",
    "        self.ffn_ln = nn.LayerNorm(hidden_dim, eps=1e-7)\n",
    "        \n",
    "    def attention_layer(self, query, key, value, attention_mask = None):\n",
    "        # output (b, s, h)\n",
    "        key = key.transpose(2, 3) # (b, head_num, head_dim, seq)\n",
    "        attention_weight = query @ key / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.tril()\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "        else:\n",
    "            attention_mask = torch.ones_like(attention_weight).tril()\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "        \n",
    "        attention_weight = torch.softmax(attention_weight, dim = -1)\n",
    "        attention_weight = self.dropout_att(attention_weight)\n",
    "        mid_out = attention_weight @ value\n",
    "        \n",
    "        \n",
    "        mid_out = mid_out.transpose(1, 2).contiguous()\n",
    "        b, s, _, _ = mid_out.size()\n",
    "        mid_out = mid_out.view(b, s, -1) # concat\n",
    "        \n",
    "        output = self.o(mid_out)\n",
    "        return output\n",
    "    \n",
    "    def mha(self, x, mask = None):\n",
    "        # (b, s, h) -> (b, head_num, s, head_dim)\n",
    "        batch, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x) # (b, s, h)\n",
    "        \n",
    "        # (b, s, h) -> (b, head_num, s, head_dim)\n",
    "        q_state = Q.view(batch, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        output = self.attention_layer(q_state, k_state, v_state, mask)\n",
    "        \n",
    "        # post norm (b, s, h)\n",
    "        return output\n",
    "    \n",
    "    def ffn(self, x):\n",
    "        up = self.up_proj(x)\n",
    "        up = self.act_fn(up)\n",
    "        down = self.down_proj(up)\n",
    "        # # dropout\n",
    "        # down = self.dropout_ffn(down)\n",
    "        # post layernorm\n",
    "        return self.ffn_ln(x + down)\n",
    "    \n",
    "    def forward(self, x, attention_mask = None):\n",
    "        # (s, h)\n",
    "        x = self.mha(x, attention_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.layer_list = nn.ModuleList([\n",
    "            SimpleDecoderLayer(64, 8) for i in range(5)\n",
    "        ])\n",
    "        \n",
    "        self.emb = nn.Embedding(12, 64)\n",
    "        self.out = nn.Linear(64, 12)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        # (3, 4)\n",
    "        x = self.emb(x)\n",
    "        for i, l in enumerate(self.layer_list):\n",
    "            x = l(x, mask)\n",
    "            print(f\"{i}th x shape\", x.shape)\n",
    "            \n",
    "        print(x.shape)\n",
    "        output = self.out(x)\n",
    "        return torch.softmax(output, dim = -1)\n",
    "        \n",
    "# (3, 4)\n",
    "x = torch.randint(low = 0, high = 12, size = (3, 4))\n",
    "print(\"x shape:\", x.shape)\n",
    "net = Decoder()\n",
    "# (3, 4) -unsqueeze-> (3, 1, 4) -unsqueeze-> (3, 1, 1, 4) -repeat-> (3, 8, 4, 4)\n",
    "mask = (\n",
    "    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 0, 0, 0]]).unsqueeze(1).unsqueeze(2).repeat(1, 8, 4, 1)\n",
    ")\n",
    "print(\"mask:\\r\\n\", mask)\n",
    "net(x, mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
