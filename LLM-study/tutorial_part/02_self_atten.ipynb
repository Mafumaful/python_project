{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写self attension\n",
    "\n",
    "## 1. 什么是self attension\n",
    "formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[[0.7795, 0.2373, 0.7946, 0.9313],\n",
      "         [0.1784, 0.8063, 0.1943, 0.1121]],\n",
      "\n",
      "        [[0.7090, 0.2948, 0.8234, 0.1098],\n",
      "         [0.6749, 0.5183, 0.0602, 0.8583]],\n",
      "\n",
      "        [[0.6374, 0.1271, 0.4514, 0.6903],\n",
      "         [0.2616, 0.2978, 0.7061, 0.7400]]])\n",
      "tensor([[[ 0.2579, -0.4916,  0.3464,  0.2177],\n",
      "         [ 0.2610, -0.4959,  0.3418,  0.2271]],\n",
      "\n",
      "        [[ 0.3481, -0.6142,  0.2398,  0.3398],\n",
      "         [ 0.3484, -0.6145,  0.2405,  0.3396]],\n",
      "\n",
      "        [[ 0.2389, -0.5029,  0.2769,  0.3219],\n",
      "         [ 0.2386, -0.5025,  0.2773,  0.3216]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simple version\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 728)->None:\n",
    "       super().__init__()\n",
    "       self.hidden_dim = hidden_dim \n",
    "       \n",
    "       self.q = nn.Linear(hidden_dim, hidden_dim)\n",
    "       self.k = nn.Linear(hidden_dim, hidden_dim)\n",
    "       self.v = nn.Linear(hidden_dim, hidden_dim)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # X shape is: (batch_size, seq_len, hidden_dim)\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        # Q K V shape (batch, seq, hidden_dim)\n",
    "        \n",
    "        # K^T\n",
    "        attention_value = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        attention_weight = torch.softmax(attention_value / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        \n",
    "        # (batch, seq, hidden_dim)\n",
    "        output = torch.matmul(attention_weight, V)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# test\n",
    "X = torch.rand(3, 2, 4)\n",
    "print(\"X:\", X)\n",
    "\n",
    "self_att_net = SelfAttentionV1(4)\n",
    "print(self_att_net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里为什么要除以$\\sqrt{d_k}$，是因为这样可以使得Q和K的点积更稳定，更不容易出现梯度消失或者梯度爆炸的情况。\n",
    "\n",
    "## 2. 如果我们要优化的话，我们可以怎么做？当网络比较小的时候，我们的怎么做？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[[0.2239, 0.4982, 0.3198, 0.2065],\n",
      "         [0.6760, 0.5683, 0.2843, 0.7094]],\n",
      "\n",
      "        [[0.4296, 0.0365, 0.9676, 0.8618],\n",
      "         [0.9661, 0.8206, 0.7621, 0.0858]],\n",
      "\n",
      "        [[0.1024, 0.1856, 0.5597, 0.9592],\n",
      "         [0.6754, 0.9313, 0.9466, 0.3101]]])\n",
      "tensor([[[ 0.4516,  0.3538, -0.3691,  0.7849],\n",
      "         [ 0.4505,  0.3566, -0.3699,  0.7883]],\n",
      "\n",
      "        [[ 0.6903,  0.1282, -0.0618,  1.0307],\n",
      "         [ 0.6926,  0.1296, -0.0609,  1.0310]],\n",
      "\n",
      "        [[ 0.5350,  0.1434, -0.2532,  0.9843],\n",
      "         [ 0.5435,  0.1446, -0.2481,  0.9878]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# efficiency optimize\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim*3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # X shape (batch, seq, dim)\n",
    "        # QKV shape(batch, seq, hidden_dim*3)\n",
    "        QKV = self.proj(x)\n",
    "        Q, K, V = torch.split(QKV, self.hidden_dim, dim=-1)\n",
    "        att_weight = torch.softmax(\n",
    "            torch.matmul(Q, K.transpose(-1, -2)),\n",
    "            dim = -1\n",
    "        ) / math.sqrt(self.hidden_dim)\n",
    "        \n",
    "        output = att_weight @ V\n",
    "        return output\n",
    "    \n",
    "# test\n",
    "X = torch.rand(3, 2, 4)\n",
    "print(\"X:\", X)\n",
    "\n",
    "self_att_net = SelfAttentionV1(4)\n",
    "print(self_att_net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加入一些细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [1, 0, 0, 0],\n",
      "         [1, 0, 0, 0],\n",
      "         [1, 0, 0, 0]]])\n",
      "X: tensor([[[0.5080, 0.5815],\n",
      "         [0.3296, 0.9273],\n",
      "         [0.5979, 0.6149],\n",
      "         [0.5742, 0.7032]],\n",
      "\n",
      "        [[0.4230, 0.2200],\n",
      "         [0.3816, 0.2268],\n",
      "         [0.0430, 0.7821],\n",
      "         [0.6195, 0.4677]],\n",
      "\n",
      "        [[0.6937, 0.2079],\n",
      "         [0.1230, 0.0851],\n",
      "         [0.1146, 0.8231],\n",
      "         [0.2967, 0.6506]]])\n"
     ]
    }
   ],
   "source": [
    "## 1. dropout position\n",
    "## 2. attention mask\n",
    "## 3. output \n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, dropout_rate: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim*3)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x, attention_mask = None):\n",
    "        QKV = self.proj(x)\n",
    "        Q, K, V = torch.split(QKV, self.hidden_dim, dim=-1)\n",
    "        \n",
    "        attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "                )\n",
    "            \n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight,\n",
    "            dim = -1\n",
    "        )\n",
    "        \n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        attention_result = attention_weight @ V\n",
    "\n",
    "        output = self.output_proj(attention_result)\n",
    "        return output\n",
    "    \n",
    "# test\n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "# print(\"mask shape \", mask.shape)\n",
    "print(mask)\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(mask)\n",
    "# print(\"mask shape \", mask.shape)\n",
    "\n",
    "print(\"X:\", X)\n",
    "\n",
    "self_att_net = SelfAttentionV3(2)\n",
    "# print(self_att_net(X, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interview oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size shape torch.Size([3, 4, 2])\n",
      "mask shape torch.Size([3, 4])\n",
      "mask shape torch.Size([3, 4, 4])\n",
      "Q shape: torch.Size([3, 4, 2])\n",
      "attention weight shape: torch.Size([3, 4, 4])\n",
      "tensor([[[-0.5785, -0.1090, -0.3257, -0.2762],\n",
      "         [-0.4806, -0.0688, -0.3072, -0.2650],\n",
      "         [-0.4174, -0.0869, -0.2212, -0.1858],\n",
      "         [-0.4043, -0.0826, -0.2168, -0.1825]],\n",
      "\n",
      "        [[-0.0487, -0.1576,  0.0632, -0.3716],\n",
      "         [-0.0673, -0.2039,  0.0792, -0.4687],\n",
      "         [-0.1082, -0.2105,  0.0579, -0.3725],\n",
      "         [-0.1546, -0.3601,  0.1178, -0.7248]],\n",
      "\n",
      "        [[-0.4661,  0.0917, -0.3320, -0.0616],\n",
      "         [-0.2817,  0.0463, -0.1862, -0.0323],\n",
      "         [-0.4904,  0.0822, -0.3266, -0.0571],\n",
      "         [-0.3798,  0.0598, -0.2467, -0.0421]]], grad_fn=<DivBackward0>)\n",
      "\n",
      " tensor([[[-0.5785, -0.1090, -0.3257,    -inf],\n",
      "         [-0.4806, -0.0688, -0.3072,    -inf],\n",
      "         [-0.4174, -0.0869, -0.2212,    -inf],\n",
      "         [-0.4043, -0.0826, -0.2168,    -inf]],\n",
      "\n",
      "        [[-0.0487, -0.1576,    -inf,    -inf],\n",
      "         [-0.0673, -0.2039,    -inf,    -inf],\n",
      "         [-0.1082, -0.2105,    -inf,    -inf],\n",
      "         [-0.1546, -0.3601,    -inf,    -inf]],\n",
      "\n",
      "        [[-0.4661,    -inf,    -inf,    -inf],\n",
      "         [-0.2817,    -inf,    -inf,    -inf],\n",
      "         [-0.4904,    -inf,    -inf,    -inf],\n",
      "         [-0.3798,    -inf,    -inf,    -inf]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionV4(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, dropout_rate: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.atten_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x, attention_mask = None):\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        \n",
    "        print(\"Q shape:\", Q.shape)\n",
    "        \n",
    "        attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        print(\"attention weight shape:\", attention_weight.shape)\n",
    "        # print(attention_mask)\n",
    "        print(attention_weight)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "        print(\"\\r\\n\", attention_weight)\n",
    "            \n",
    "        # (batch, seq, seq)\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight,\n",
    "            dim = -1\n",
    "        )\n",
    "        \n",
    "        attention_weight = self.atten_dropout(attention_weight)\n",
    "        output = attention_weight @ V\n",
    "\n",
    "        # (batch, seq, dim)\n",
    "        return output\n",
    "\n",
    "# test\n",
    "X = torch.rand(3, 4, 2)\n",
    "print(\"X size shape\", X.shape)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "print(\"mask shape\", mask.shape)\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(\"mask shape\", mask.shape)\n",
    "\n",
    "self_att_net = SelfAttentionV4(2)\n",
    "result = self_att_net(X, mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
