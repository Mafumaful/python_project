{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写self attension\n",
    "\n",
    "## 1. 什么是self attension\n",
    "formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[[0.4821, 0.5496, 0.2873, 0.6103],\n",
      "         [0.7172, 0.1542, 0.7106, 0.2280]],\n",
      "\n",
      "        [[0.4413, 0.1183, 0.5076, 0.6402],\n",
      "         [0.5094, 0.3109, 0.7545, 0.1079]],\n",
      "\n",
      "        [[0.6474, 0.8568, 0.4717, 0.9785],\n",
      "         [0.0347, 0.8786, 0.8726, 0.8526]]])\n",
      "tensor([[[-0.4421,  0.1213, -0.3098,  0.0249],\n",
      "         [-0.4422,  0.1214, -0.3098,  0.0249]],\n",
      "\n",
      "        [[-0.3942, -0.0236, -0.1933,  0.0258],\n",
      "         [-0.3940, -0.0229, -0.1918,  0.0274]],\n",
      "\n",
      "        [[-0.7986,  0.2971,  0.0608, -0.0584],\n",
      "         [-0.7986,  0.2971,  0.0608, -0.0584]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simple version\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 728)->None:\n",
    "       super().__init__()\n",
    "       self.hidden_dim = hidden_dim \n",
    "       \n",
    "       self.q = nn.Linear(hidden_dim, hidden_dim)\n",
    "       self.k = nn.Linear(hidden_dim, hidden_dim)\n",
    "       self.v = nn.Linear(hidden_dim, hidden_dim)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # X shape is: (batch_size, seq_len, hidden_dim)\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        # Q K V shape (batch, seq, hidden_dim)\n",
    "        \n",
    "        # K^T\n",
    "        attention_value = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        attention_weight = torch.softmax(attention_value / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        \n",
    "        # (batch, seq, hidden_dim)\n",
    "        output = torch.matmul(attention_weight, V)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# test\n",
    "X = torch.rand(3, 2, 4)\n",
    "print(\"X:\", X)\n",
    "\n",
    "self_att_net = SelfAttentionV1(4)\n",
    "print(self_att_net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里为什么要除以$\\sqrt{d_k}$，是因为这样可以使得Q和K的点积更稳定，更不容易出现梯度消失或者梯度爆炸的情况。\n",
    "\n",
    "## 2. 如果我们要优化的话，我们可以怎么做？当网络比较小的时候，我们的怎么做？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[[0.1162, 0.8403, 0.3770, 0.5865],\n",
      "         [0.0939, 0.2697, 0.4536, 0.6432]],\n",
      "\n",
      "        [[0.8119, 0.2099, 0.7808, 0.2939],\n",
      "         [0.0445, 0.3325, 0.1797, 0.2189]],\n",
      "\n",
      "        [[0.1676, 0.0918, 0.4949, 0.8680],\n",
      "         [0.7305, 0.5256, 0.3589, 0.2128]]])\n",
      "tensor([[[-0.2721,  0.2719, -0.0715, -0.2825],\n",
      "         [-0.2718,  0.2717, -0.0714, -0.2830]],\n",
      "\n",
      "        [[-0.1489,  0.1622, -0.1168, -0.1334],\n",
      "         [-0.1489,  0.1622, -0.1165, -0.1342]],\n",
      "\n",
      "        [[-0.2043,  0.2489, -0.1393, -0.0328],\n",
      "         [-0.2276,  0.2667, -0.1377, -0.0190]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# efficiency optimize\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim*3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # X shape (batch, seq, dim)\n",
    "        # QKV shape(batch, seq, hidden_dim*3)\n",
    "        QKV = self.proj(x)\n",
    "        Q, K, V = torch.split(QKV, self.hidden_dim, dim=-1)\n",
    "        att_weight = torch.softmax(\n",
    "            torch.matmul(Q, K.transpose(-1, -2)),\n",
    "            dim = -1\n",
    "        ) / math.sqrt(self.hidden_dim)\n",
    "        \n",
    "        output = att_weight @ V\n",
    "        return output\n",
    "    \n",
    "# test\n",
    "X = torch.rand(3, 2, 4)\n",
    "print(\"X:\", X)\n",
    "\n",
    "self_att_net = SelfAttentionV1(4)\n",
    "print(self_att_net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加入一些细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape  torch.Size([3, 4])\n",
      "mask shape  torch.Size([3, 4, 4])\n",
      "X: tensor([[[0.8306, 0.1741],\n",
      "         [0.2105, 0.7246],\n",
      "         [0.8159, 0.0152],\n",
      "         [0.5970, 0.1283]],\n",
      "\n",
      "        [[0.4402, 0.6248],\n",
      "         [0.8392, 0.3145],\n",
      "         [0.0983, 0.4623],\n",
      "         [0.8232, 0.2566]],\n",
      "\n",
      "        [[0.9905, 0.2713],\n",
      "         [0.9090, 0.8953],\n",
      "         [0.7381, 0.6693],\n",
      "         [0.4292, 0.7702]]])\n",
      "tensor([[[ 0.5422, -0.0870],\n",
      "         [-0.3529,  1.8475],\n",
      "         [ 0.6292, -0.2785],\n",
      "         [ 0.1152,  0.8339]],\n",
      "\n",
      "        [[ 0.6292, -0.2785],\n",
      "         [ 0.3301,  0.3729],\n",
      "         [ 0.6292, -0.2785],\n",
      "         [-0.0797,  1.2573]],\n",
      "\n",
      "        [[-0.8814,  2.9921],\n",
      "         [ 0.6292, -0.2785],\n",
      "         [ 0.6292, -0.2785],\n",
      "         [-0.8814,  2.9921]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## 1. dropout position\n",
    "## 2. attention mask\n",
    "## 3. output \n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, dropout_rate: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim*3)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x, attention_mask = None):\n",
    "        QKV = self.proj(x)\n",
    "        Q, K, V = torch.split(QKV, self.hidden_dim, dim=-1)\n",
    "        \n",
    "        attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "                )\n",
    "            \n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight,\n",
    "            dim = -1\n",
    "        )\n",
    "        \n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        attention_result = attention_weight @ V\n",
    "\n",
    "        output = self.output_proj(attention_result)\n",
    "        return output\n",
    "    \n",
    "# test\n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "print(\"mask shape \", mask.shape)\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(\"mask shape \", mask.shape)\n",
    "\n",
    "print(\"X:\", X)\n",
    "\n",
    "self_att_net = SelfAttentionV3(2)\n",
    "print(self_att_net(X, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interview oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape  torch.Size([3, 4])\n",
      "mask shape  torch.Size([3, 4, 4])\n",
      "X: tensor([[[0.4089, 0.7366],\n",
      "         [0.0619, 0.2798],\n",
      "         [0.7809, 0.8408],\n",
      "         [0.1825, 0.4636]],\n",
      "\n",
      "        [[0.9935, 0.9957],\n",
      "         [0.9282, 0.5119],\n",
      "         [0.9985, 0.7510],\n",
      "         [0.0298, 0.7328]],\n",
      "\n",
      "        [[0.2781, 0.8414],\n",
      "         [0.0555, 0.3954],\n",
      "         [0.5185, 0.9357],\n",
      "         [0.6754, 0.0191]]])\n",
      "tensor([[[-0.4380, -0.3482],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 1.3571, -0.6325]],\n",
      "\n",
      "        [[ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionV4(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, dropout_rate: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.atten_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x, attention_mask = None):\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        \n",
    "        attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "            \n",
    "        # (batch, seq, seq)\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight,\n",
    "            dim = -1\n",
    "        )\n",
    "        \n",
    "        attention_weight = self.atten_dropout(attention_weight)\n",
    "        output = attention_weight @ V\n",
    "\n",
    "        # (batch, seq, dim)\n",
    "        return output\n",
    "\n",
    "# test\n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "print(\"mask shape \", mask.shape)\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(\"mask shape \", mask.shape)\n",
    "\n",
    "print(\"X:\", X)\n",
    "\n",
    "self_att_net = SelfAttentionV4(2)\n",
    "print(self_att_net(X, mask))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
