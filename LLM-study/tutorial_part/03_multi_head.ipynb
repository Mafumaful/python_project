{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Self-Attention\n",
    "![self_multi_attention](./../image/self_multi_atten.png)\n",
    "\n",
    "上面的self attention其实就是描述的\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "这个公式。\n",
    "\n",
    "MultiHead attention可以参考：\n",
    "$$ MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O $$\n",
    "其中\n",
    "$$ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0471,  0.1656,  0.2075,  0.0242, -0.0959, -0.3411,  0.1504,\n",
       "          -0.4096,  0.2382,  0.2489,  0.2209,  0.1893, -0.0222,  0.1877,\n",
       "          -0.2426, -0.2405,  0.2585, -0.3117, -0.1795,  0.0900,  0.5275,\n",
       "           0.3705, -0.1212, -0.2062,  0.4271, -0.2557,  0.1747, -0.1633,\n",
       "          -0.1529, -0.0054, -0.1377,  0.2733, -0.4722,  0.1235, -0.1139,\n",
       "          -0.2334,  0.2540, -0.3412,  0.1857, -0.0594,  0.1311,  0.0223,\n",
       "           0.0512,  0.0360, -0.0992,  0.2679,  0.1296,  0.3204, -0.4174,\n",
       "           0.2088,  0.1322, -0.0610,  0.1558, -0.1672,  0.1751, -0.3704,\n",
       "           0.0196, -0.0290, -0.2155, -0.1648,  0.0823,  0.1269,  0.3863,\n",
       "          -0.3423, -0.1285,  0.0373, -0.1913, -0.1605, -0.4552,  0.2384,\n",
       "           0.0624,  0.0482,  0.0336, -0.0083, -0.3587, -0.1341,  0.1184,\n",
       "           0.7127,  0.4748,  0.1044,  0.1637, -0.1446, -0.0902,  0.0372,\n",
       "           0.1216, -0.3071,  0.2499,  0.1505,  0.0847, -0.2407,  0.3632,\n",
       "           0.1685, -0.2675, -0.2420, -0.0836, -0.0638, -0.0320, -0.4367,\n",
       "           0.1618,  0.0016,  0.3218,  0.1967, -0.0544, -0.1260,  0.0846,\n",
       "           0.0028, -0.2983, -0.4578, -0.2964, -0.0504, -0.1725, -0.0856,\n",
       "          -0.3108,  0.1807,  0.3444, -0.0159, -0.0643,  0.5509, -0.1322,\n",
       "          -0.3823,  0.0038,  0.0475, -0.1626,  0.1529, -0.2667,  0.3709,\n",
       "          -0.1301,  0.0893],\n",
       "         [ 0.0475, -0.0277,  0.3342,  0.1682,  0.0139, -0.1751,  0.1007,\n",
       "          -0.0914,  0.2906,  0.0172,  0.1758,  0.0727,  0.0897,  0.0815,\n",
       "          -0.3890, -0.1197,  0.3016, -0.1073, -0.2115,  0.1347,  0.4615,\n",
       "           0.2377, -0.1631, -0.2735,  0.4080, -0.3889,  0.0749, -0.0027,\n",
       "          -0.2542,  0.0432,  0.0166,  0.2107, -0.5120,  0.3301, -0.1317,\n",
       "          -0.1221,  0.2901, -0.2013,  0.2107, -0.1701,  0.2778,  0.3582,\n",
       "           0.0483,  0.0242, -0.1891,  0.3345,  0.1768,  0.1419, -0.3011,\n",
       "           0.1850, -0.0113, -0.1011,  0.0994, -0.1850,  0.0885, -0.2872,\n",
       "           0.0490,  0.0749, -0.2631, -0.1188, -0.1181,  0.0850,  0.3081,\n",
       "          -0.0430,  0.0565,  0.0539, -0.0344, -0.3103, -0.2915,  0.1175,\n",
       "           0.1532,  0.1881,  0.1082,  0.0127, -0.4524, -0.1376,  0.0755,\n",
       "           0.5808,  0.3829,  0.1653,  0.0647, -0.2525, -0.1284,  0.0760,\n",
       "          -0.0099, -0.0981,  0.2904,  0.0476,  0.0325, -0.1519,  0.1658,\n",
       "           0.1549, -0.2679, -0.2098, -0.0913, -0.0766,  0.0653, -0.3744,\n",
       "           0.2567, -0.1137,  0.1793,  0.1267, -0.0473, -0.2946,  0.1153,\n",
       "           0.1699, -0.2873, -0.3848, -0.2553, -0.2361, -0.0099, -0.1402,\n",
       "          -0.2513,  0.0800,  0.3049, -0.0539, -0.0833,  0.3081,  0.0681,\n",
       "          -0.3102, -0.0519,  0.2478, -0.1163,  0.0285, -0.1717,  0.2169,\n",
       "          -0.1918,  0.0465]],\n",
       "\n",
       "        [[    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan]],\n",
       "\n",
       "        [[-0.2249,  0.1828,  0.1808, -0.0217,  0.0229, -0.1961,  0.2432,\n",
       "          -0.2356,  0.2223,  0.1362,  0.2125, -0.0072, -0.0589,  0.0347,\n",
       "          -0.4709, -0.1082,  0.3153, -0.0785, -0.1533, -0.0526,  0.3735,\n",
       "           0.2724, -0.1749, -0.2972,  0.3278, -0.1595,  0.0787,  0.0091,\n",
       "          -0.2905,  0.0739, -0.0228,  0.1005, -0.3452,  0.0294,  0.0890,\n",
       "          -0.0458,  0.4334, -0.1901,  0.0791, -0.3354,  0.3161,  0.4324,\n",
       "           0.0766, -0.0700, -0.1359,  0.1497,  0.2743,  0.0693, -0.4389,\n",
       "           0.1080,  0.0744, -0.1488,  0.1885, -0.1413,  0.3318, -0.1747,\n",
       "           0.0143, -0.0065, -0.1683, -0.0903,  0.0201, -0.0109,  0.2184,\n",
       "          -0.0137, -0.0086,  0.0137, -0.1882, -0.2142, -0.1979,  0.1620,\n",
       "           0.0727,  0.0140,  0.0142,  0.2336, -0.2624, -0.0392,  0.0741,\n",
       "           0.4245,  0.4253,  0.1499, -0.0078, -0.0630, -0.0668,  0.0891,\n",
       "           0.1416, -0.0958,  0.1863, -0.0514, -0.0885, -0.0963,  0.3777,\n",
       "           0.0052, -0.0718, -0.1611, -0.1307, -0.0956,  0.1414, -0.1432,\n",
       "           0.2269, -0.1354,  0.0633,  0.0903, -0.0209, -0.2590,  0.1424,\n",
       "          -0.1105, -0.1118, -0.3188, -0.2613,  0.0041,  0.0418,  0.0148,\n",
       "          -0.1473,  0.1570,  0.2102, -0.2605, -0.1867,  0.3154, -0.0789,\n",
       "          -0.2964, -0.2369,  0.0371, -0.3200,  0.0618,  0.0087,  0.1711,\n",
       "          -0.1470, -0.2019],\n",
       "         [-0.3672,  0.2992,  0.0351, -0.1573, -0.1169, -0.2208,  0.3612,\n",
       "          -0.2092,  0.3420,  0.2034,  0.2114,  0.1033, -0.1090,  0.1411,\n",
       "          -0.3950,  0.0014,  0.3663, -0.2628, -0.0350,  0.2109,  0.1632,\n",
       "           0.1851, -0.1637, -0.0484,  0.4499, -0.1044, -0.0119, -0.1111,\n",
       "           0.0217, -0.0731, -0.1817,  0.2222, -0.1101,  0.1002,  0.1100,\n",
       "          -0.1172,  0.2864, -0.3338,  0.1087, -0.1419,  0.2753,  0.3619,\n",
       "           0.2637, -0.1848, -0.1795,  0.0274,  0.3165,  0.0783, -0.2659,\n",
       "           0.2150, -0.0077,  0.0349,  0.0490, -0.2168,  0.2648, -0.1727,\n",
       "           0.0994, -0.0270,  0.0775, -0.0576,  0.2698,  0.2176,  0.1287,\n",
       "          -0.2487, -0.0926, -0.1408, -0.1049,  0.0264, -0.2033,  0.1441,\n",
       "           0.0534, -0.0712,  0.0180,  0.2321, -0.2839, -0.0723,  0.1915,\n",
       "           0.2611,  0.2501, -0.1597, -0.0789, -0.0858,  0.1036,  0.2251,\n",
       "           0.4560, -0.1717,  0.2021,  0.0035, -0.1843, -0.1003,  0.4244,\n",
       "          -0.0540, -0.1468, -0.1804, -0.0132, -0.2061,  0.1386, -0.1867,\n",
       "           0.2889, -0.0040,  0.1831, -0.0454, -0.0084, -0.0713, -0.0506,\n",
       "          -0.2644, -0.0810, -0.2605, -0.3270,  0.0767, -0.0072, -0.0655,\n",
       "          -0.2372,  0.3487,  0.2644, -0.2509, -0.1749,  0.1606, -0.2392,\n",
       "          -0.2739, -0.1209, -0.0944, -0.2485, -0.0617, -0.1206,  0.3328,\n",
       "           0.0737, -0.0099]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAtten(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, attention_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.head_num = head_num\n",
    "        \n",
    "        self.q = nn.Linear(hidden_dim, hidden_dim) # (hidden_dim, head_dim * head_num)\n",
    "        self.k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "    def forward(self, x, attention_mask = None):\n",
    "        batch, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x) # (b, s, h)\n",
    "        \n",
    "        # (b, s, h) -> (b, head_num, s, head_dim)\n",
    "        q_state = Q.view(batch, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # (b, head_num, s, s)\n",
    "        attention_weight = q_state @ k_state.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float('-inf')\n",
    "            )\n",
    "        # print(attention_weight)\n",
    "        \n",
    "        attention_weight = torch.softmax(attention_weight, dim = -1)\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        # (b, head_num, s, s) @ (b, head_num, s, head_dim) -> (b, head_num, s, head_dim)\n",
    "        output_mid = attention_weight @ v_state \n",
    "        \n",
    "        # (b, head_num, s, head_dim) -transpose-> (b, s, head_num, head_dim)\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous() # 内存连续化\n",
    "        # (b, s, head_num, head_dim) -view-> (b, s, hidden_dim)\n",
    "        output_mid = output_mid.view(batch, seq_len, -1)\n",
    "        \n",
    "        output = self.out_proj(output_mid)\n",
    "        return output\n",
    "    \n",
    "# test\n",
    "X = torch.rand(3, 2, 128)\n",
    "# mast (3, 2) -unsqueeze-> (3, 1, 2) -unsqueeze-> (3, 1, 1, 2) -expand-> (3, 8, 2, 2)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [0, 1],\n",
    "        [0, 0],\n",
    "        [1, 0]\n",
    "    ]\n",
    ").unsqueeze(1).unsqueeze(2).expand(3, 8, 2, 2)\n",
    "\n",
    "# head number: 8\n",
    "att_net = MultiHeadSelfAtten(128, 8)\n",
    "out = att_net(X, mask)\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
