{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miakho/miniconda3/envs/lmdrive/lib/python3.8/site-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "\n",
    "dataset = OGB_MAG(root='./data', preprocess='metapath2vec')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  paper={\n",
       "    x=[736389, 128],\n",
       "    year=[736389],\n",
       "    y=[736389],\n",
       "    train_mask=[736389],\n",
       "    val_mask=[736389],\n",
       "    test_mask=[736389],\n",
       "  },\n",
       "  author={ x=[1134649, 128] },\n",
       "  institution={ x=[8740, 128] },\n",
       "  field_of_study={ x=[59965, 128] },\n",
       "  (author, affiliated_with, institution)={ edge_index=[2, 1043998] },\n",
       "  (author, writes, paper)={ edge_index=[2, 7145660] },\n",
       "  (paper, cites, paper)={ edge_index=[2, 5416271] },\n",
       "  (paper, has_topic, field_of_study)={ edge_index=[2, 7505078] }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[-0.0954,  0.0408, -0.2109,  ...,  0.0616, -0.0277, -0.1338],\n",
       "        [-0.1510, -0.1073, -0.2220,  ...,  0.3458, -0.0277, -0.2185],\n",
       "        [-0.1148, -0.1760, -0.2606,  ...,  0.1731, -0.1564, -0.2780],\n",
       "        ...,\n",
       "        [ 0.0228, -0.0865,  0.0981,  ..., -0.0547, -0.2077, -0.2305],\n",
       "        [-0.2891, -0.2029, -0.1525,  ...,  0.1042,  0.2041, -0.3528],\n",
       "        [-0.0890, -0.0348, -0.2642,  ...,  0.2601, -0.0875, -0.5171]]), 'year': tensor([2015, 2012, 2012,  ..., 2016, 2017, 2014]), 'y': tensor([246, 131, 189,  ..., 266, 289,   1]), 'train_mask': tensor([True, True, True,  ..., True, True, True]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ..., False, False, False])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_node_data = data['paper']\n",
    "paper_node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[     0,      0,      0,  ..., 736388, 736388, 736388],\n",
       "        [    88,  27449, 121051,  ..., 421711, 427339, 439864]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cites_edge_data = data['paper', 'cites', 'paper']\n",
    "cites_edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_index': tensor([[     0,      0,      0,  ..., 736388, 736388, 736388],\n",
      "        [   145,   2215,   3205,  ...,  21458,  22283,  31934]])}\n"
     ]
    }
   ],
   "source": [
    "print(data['has_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'author', 'institution', 'field_of_study']\n",
      "[('author', 'affiliated_with', 'institution'), ('author', 'writes', 'paper'), ('paper', 'cites', 'paper'), ('paper', 'has_topic', 'field_of_study')]\n"
     ]
    }
   ],
   "source": [
    "node_types, edge_types = data.metadata()\n",
    "print(node_types)\n",
    "print(edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(data.has_isolated_nodes())\n",
    "print(data.has_self_loops())\n",
    "print(data.is_undirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch\n",
    "\n",
    "\n",
    "dataset = OGB_MAG(root='./data', preprocess='metapath2vec', transform=T.ToUndirected())\n",
    "data = dataset[0]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GNN(hidden_channels=64, out_channels=dataset.num_classes)\n",
    "model = to_hetero(model, data.metadata(), aggr='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "# F\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    mask = data['paper'].train_mask\n",
    "    loss = F.cross_entropy(out['paper'][mask], data['paper'].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 001, Loss: 6.1408\n",
      "Epoch: 002, Loss: 5.0984\n",
      "Epoch: 003, Loss: 4.8924\n",
      "Epoch: 004, Loss: 4.5672\n",
      "Epoch: 005, Loss: 4.4128\n",
      "Epoch: 006, Loss: 4.1781\n",
      "Epoch: 007, Loss: 3.9986\n",
      "Epoch: 008, Loss: 3.8098\n",
      "Epoch: 009, Loss: 3.6439\n",
      "Epoch: 010, Loss: 3.4645\n",
      "Epoch: 011, Loss: 3.3149\n",
      "Epoch: 012, Loss: 3.1838\n",
      "Epoch: 013, Loss: 3.0677\n",
      "Epoch: 014, Loss: 2.9811\n",
      "Epoch: 015, Loss: 2.9024\n",
      "Epoch: 016, Loss: 2.8423\n",
      "Epoch: 017, Loss: 2.7892\n",
      "Epoch: 018, Loss: 2.7378\n",
      "Epoch: 019, Loss: 2.6955\n",
      "Epoch: 020, Loss: 2.6659\n",
      "Epoch: 021, Loss: 2.6290\n",
      "Epoch: 022, Loss: 2.5993\n",
      "Epoch: 023, Loss: 2.5729\n",
      "Epoch: 024, Loss: 2.5459\n",
      "Epoch: 025, Loss: 2.5206\n",
      "Epoch: 026, Loss: 2.4973\n",
      "Epoch: 027, Loss: 2.4754\n",
      "Epoch: 028, Loss: 2.4554\n",
      "Epoch: 029, Loss: 2.4370\n",
      "Epoch: 030, Loss: 2.4192\n",
      "Epoch: 031, Loss: 2.4029\n",
      "Epoch: 032, Loss: 2.3866\n",
      "Epoch: 033, Loss: 2.3716\n",
      "Epoch: 034, Loss: 2.3570\n",
      "Epoch: 035, Loss: 2.3416\n",
      "Epoch: 036, Loss: 2.3282\n",
      "Epoch: 037, Loss: 2.3155\n",
      "Epoch: 038, Loss: 2.3023\n",
      "Epoch: 039, Loss: 2.2900\n",
      "Epoch: 040, Loss: 2.2785\n",
      "Epoch: 041, Loss: 2.2668\n",
      "Epoch: 042, Loss: 2.2559\n",
      "Epoch: 043, Loss: 2.2451\n",
      "Epoch: 044, Loss: 2.2349\n",
      "Epoch: 045, Loss: 2.2248\n",
      "Epoch: 046, Loss: 2.2155\n",
      "Epoch: 047, Loss: 2.2063\n",
      "Epoch: 048, Loss: 2.1984\n",
      "Epoch: 049, Loss: 2.1927\n",
      "Epoch: 050, Loss: 2.1930\n",
      "Epoch: 051, Loss: 2.1868\n",
      "Epoch: 052, Loss: 2.1699\n",
      "Epoch: 053, Loss: 2.1590\n",
      "Epoch: 054, Loss: 2.1589\n",
      "Epoch: 055, Loss: 2.1509\n",
      "Epoch: 056, Loss: 2.1400\n",
      "Epoch: 057, Loss: 2.1379\n",
      "Epoch: 058, Loss: 2.1293\n",
      "Epoch: 059, Loss: 2.1216\n",
      "Epoch: 060, Loss: 2.1196\n",
      "Epoch: 061, Loss: 2.1102\n",
      "Epoch: 062, Loss: 2.1041\n",
      "Epoch: 063, Loss: 2.1012\n",
      "Epoch: 064, Loss: 2.0918\n",
      "Epoch: 065, Loss: 2.0882\n",
      "Epoch: 066, Loss: 2.0834\n",
      "Epoch: 067, Loss: 2.0743\n",
      "Epoch: 068, Loss: 2.0728\n",
      "Epoch: 069, Loss: 2.0659\n",
      "Epoch: 070, Loss: 2.0583\n",
      "Epoch: 071, Loss: 2.0570\n",
      "Epoch: 072, Loss: 2.0488\n",
      "Epoch: 073, Loss: 2.0444\n",
      "Epoch: 074, Loss: 2.0409\n",
      "Epoch: 075, Loss: 2.0345\n",
      "Epoch: 076, Loss: 2.0314\n",
      "Epoch: 077, Loss: 2.0257\n",
      "Epoch: 078, Loss: 2.0211\n",
      "Epoch: 079, Loss: 2.0194\n",
      "Epoch: 080, Loss: 2.0146\n",
      "Epoch: 081, Loss: 2.0128\n",
      "Epoch: 082, Loss: 2.0131\n",
      "Epoch: 083, Loss: 2.0096\n",
      "Epoch: 084, Loss: 2.0050\n",
      "Epoch: 085, Loss: 1.9958\n",
      "Epoch: 086, Loss: 1.9895\n",
      "Epoch: 087, Loss: 1.9881\n",
      "Epoch: 088, Loss: 1.9869\n",
      "Epoch: 089, Loss: 1.9827\n",
      "Epoch: 090, Loss: 1.9745\n",
      "Epoch: 091, Loss: 1.9695\n",
      "Epoch: 092, Loss: 1.9686\n",
      "Epoch: 093, Loss: 1.9660\n",
      "Epoch: 094, Loss: 1.9612\n",
      "Epoch: 095, Loss: 1.9561\n",
      "Epoch: 096, Loss: 1.9532\n",
      "Epoch: 097, Loss: 1.9516\n",
      "Epoch: 098, Loss: 1.9489\n",
      "Epoch: 099, Loss: 1.9466\n",
      "Epoch: 100, Loss: 1.9477\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "print('Training...')\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([146, 291, 189,  ...,  83,   9,   1])\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "input = data.x_dict\n",
    "out = model(input, data.edge_index_dict)\n",
    "print(out['paper'].argmax(dim=-1))\n",
    "\n",
    "# evaluate\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    mask = data['paper'].test_mask\n",
    "    pred = out['paper'][mask].argmax(dim=-1)\n",
    "    y = data['paper'].y[mask]\n",
    "    acc = pred.eq(y).to(torch.float).mean().item()\n",
    "    return acc\n",
    "\n",
    "acc = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4225899577140808"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
